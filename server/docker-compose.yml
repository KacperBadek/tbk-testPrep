version: '3.8'
services:
  mongodb:
    image: mongo:latest
    container_name: TEST
    ports:
      - "27020:27017"
    volumes:
      - mongodb_data:/data/db
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example

volumes:
  mongodb_data:

#docker cp "C:/Users/user/Desktop/Aplikacje internetowe/tbk-testPrep/server/movies.json" TEST:/movies.json
#docker exec -i TEST mongoimport --username root --password example --authenticationDatabase admin --db mongo_db_test --collection movies --file /movies.json --jsonArray

#docker cp "C:/Users/user/Desktop/Aplikacje internetowe/tbk-testPrep/server/movies.csv" TEST:/movies.csv
#docker exec -i TEST mongoimport --username root --password example --authenticationDatabase admin --db mongo_db_test --collection movies --type csv --file /movies.csv --headerline
#BRAK NAGŁÓWKÓW docker exec -i TEST mongoimport --username root --password example --authenticationDatabase admin --db mongo_db_test --collection movies --type csv --file /movies.csv --fields title,director,genre,releaseYear,rating

#  const initDb = async (req, res, next) => {
#  try {
#// 1. Najpierw wyczyszczamy kolekcję:
#  await Movie.deleteMany();
#
#// 2. Budujemy ścieżkę do dużego pliku JSON (zawierającego tablicę filmów):
#  //    Upewnij się, że plik faktycznie istnieje w podanej lokalizacji.
#  //!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! - teraz przestawiam na wygenerwoane
#  // const filePath = path.join(__dirname, '..', 'data', 'data.json');
#  const filePath = path.join(__dirname, '..', 'data', 'generated.json');
#
#  // 3. Tworzymy strumień odczytu z pliku i parser JSONStream,
#  //    który będzie przetwarzał każdy element tablicy osobno.
#const readStream = fs.createReadStream(filePath, { encoding: 'utf8' });
#  const parser = JSONStream.parse(''); // '' = każdy element tablicy w pliku
#
#// 4. Przygotowujemy zmienne do batchowania:
#  let docsBatch = [];
#  const BATCH_SIZE = 500; // np. co 500 dokumentów zapisuj do bazy
#  let insertedCount = 0;
#
#// 5. Nasłuchujemy zdarzenia "data" — za każdym razem, gdy JSONStream odczyta kolejny obiekt:
#  parser.on('data', async (doc) => {
#// Dodaj obiekt do paczki:
#  docsBatch.push(doc);
#
#  // Jeśli paczka osiągnęła BATCH_SIZE, wstawiamy ją do Mongo i resetujemy.
#  if (docsBatch.length >= BATCH_SIZE) {
#// Wstrzymujemy parser, żeby nie nadchodziły kolejne obiekty w trakcie zapisu:
#  parser.pause();
#  await Movie.insertMany(docsBatch);
#  insertedCount += docsBatch.length;
#  docsBatch = [];
#// Wznawiamy wczytywanie:
#  parser.resume();
#}
#});
#
#// 6. Zdarzenie "end" nastąpi, gdy parser przerobi cały plik:
#  parser.on('end', async () => {
#// Wstawiamy ostatnią paczkę, jeśli coś w niej zostało:
#  if (docsBatch.length) {
#  await Movie.insertMany(docsBatch);
#  insertedCount += docsBatch.length;
#}
#// Zwracamy odpowiedź JSON:
#  return res.json({
#message: 'Movies DB inited successfully (streaming)',
#  insertedCount
#});
#});
#
#  // 7. Obsługa błędów parsera — wywołujemy next(err), żeby nasz globalny errorHandler je przechwycił.
#  parser.on('error', (err) => {
#  next(err);
#});
#
#  // 8. Łączymy strumień z parserem — to rozpoczyna proces wczytywania i przetwarzania danych.
#  readStream.pipe(parser);
#
#} catch (err) {
#  next(err);
#}
#}